{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd40f4f4-5a80-40b7-94b3-aed136daa9d8",
   "metadata": {},
   "source": [
    "# Process Load Data for the GODEEEP Project\n",
    "\n",
    "This notebook merges together the time series of hourly electricity demand from transportation and non-transportation sources by Balancing Authority (BA) and produces the output in a format ready to ingest into GridView."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e597e9-538f-41d4-afa9-a268b10ab1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the packages we need:\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f6fe5-0769-4298-887e-7c4cffc64a0d",
   "metadata": {},
   "source": [
    "## Set the Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd887600-32da-46dc-b411-fa63c75f564f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the data input and output directories:\n",
    "trans_data_input_dir =  '/Users/burl878/Documents/GODEEEP/Data/Transportation/Raw/'\n",
    "tell_data_input_dir =  '/Users/burl878/Documents/GODEEEP/Data/TELL/Production_Runs/tell_data/outputs/tell_output/'\n",
    "merged_data_output_dir =  '/Users/burl878/Documents/GODEEEP/Data/Merged_BA_Loads/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a61439a-3f17-4e5c-b530-210b9d9d5afd",
   "metadata": {},
   "source": [
    "## Set the Scenario and Year You Want to Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2975ddb3-1dee-4bc0-becc-888849279d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcam_scenario_to_process = 'BAU_Climate'\n",
    "year_to_process = '2050'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169dc070-0ab8-4833-b6a0-589869ebf64e",
   "metadata": {},
   "source": [
    "## Merge the Transportation Output Files Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6ccf33-f406-4fb7-9e16-32e32e2225dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_transportation_data(gcam_scenario_to_process: str, year_to_process: str, trans_data_input_dir: str):\n",
    "   \n",
    "    # Create a list of all of the transportation output files in the \"trans_data_input_dir\" and aggregate the files in that list:\n",
    "    trans_filelist = sorted(glob.glob(os.path.join(trans_data_input_dir, gcam_scenario_to_process, 'rcp85hotter', ('*' + year_to_process + '.csv'))))\n",
    "       \n",
    "    # Loop over the list of files:\n",
    "    for file in range(len(trans_filelist)):\n",
    "        # Read in the .csv file:\n",
    "        trn_data = pd.read_csv(trans_filelist[file])\n",
    "\n",
    "        # Rename a few variables for consistency with TELL:\n",
    "        trn_data.rename(columns={'balancing_authority': 'BA_Code',\n",
    "                                 'time': 'Time_UTC',\n",
    "                                 'transportation_load_MWh': 'Transportation_Load_MWh'}, inplace=True)\n",
    "           \n",
    "        # Shift the timestampe by a single hour:\n",
    "        trn_data['Time_UTC'] = (pd.to_datetime(trn_data['Time_UTC']) - pd.DateOffset(hours=1)).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Only keep the columns that are needed:\n",
    "        trn_data = trn_data[['BA_Code', 'Time_UTC', 'Transportation_Load_MWh']].copy()\n",
    "        \n",
    "        # Strip the \"+00:00\" from the time string:\n",
    "        trn_data['Time_UTC'] = trn_data['Time_UTC'].astype(str)\n",
    "        trn_data['Time_UTC'] = trn_data['Time_UTC'].str.split('+').str[0]\n",
    "\n",
    "        # Aggregate the output into a new dataframe:\n",
    "        if file == 0:\n",
    "           trn_output_df = trn_data\n",
    "        else:\n",
    "           trn_output_df = pd.concat([trn_output_df, trn_data])\n",
    "      \n",
    "        # Clean up and move to the next file:\n",
    "        del trn_data\n",
    "           \n",
    "    return trn_output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f92d48-d4f0-482a-a2cc-207592369c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the transportation output files into a single dataframe:\n",
    "trn_df = merge_transportation_data(gcam_scenario_to_process = gcam_scenario_to_process, \n",
    "                                   year_to_process = year_to_process, \n",
    "                                   trans_data_input_dir = trans_data_input_dir)\n",
    "\n",
    "# Preview the transportation dataframe:\n",
    "trn_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6b815-fcbf-4d6f-b4f8-4f8b3881cfc1",
   "metadata": {},
   "source": [
    "## Read in the TELL Output File and Subset to Just the BAs in the WECC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c6000a-69e3-434b-b63f-dffeca8d75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the BA-to-Interconnection mapping file:\n",
    "ba_mapping_df = pd.read_csv(merged_data_output_dir + 'BA_to_Interconnection_Mapping.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a330d0c1-a657-4365-bf77-0bb8ea4c58c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the TELL BA output file for the year and scenario being processed:\n",
    "tell_df = pd.read_csv(tell_data_input_dir + gcam_scenario_to_process + '/' + year_to_process + '/TELL_Balancing_Authority_Hourly_Load_Data_' + year_to_process + '_Scaled_' + year_to_process + '.csv')\n",
    "\n",
    "# Rename a few variables for consistency:\n",
    "tell_df.rename(columns={'Scaled_TELL_BA_Load_MWh': 'Non-Transportation_Load_MWh'}, inplace=True)\n",
    "\n",
    "# Merge the ba_mapping_df and tell_df dataframes based on common \"BA_Code\":\n",
    "tell_df = tell_df.merge(ba_mapping_df, on=['BA_Code'])\n",
    "\n",
    "# Subset to just the WECC BAs:\n",
    "tell_df = tell_df[tell_df['Interconnection'] == 'WECC'].copy()\n",
    "\n",
    "# Make the time variable a string:\n",
    "tell_df['Time_UTC'] = tell_df['Time_UTC'].astype(str)\n",
    "\n",
    "# Only keep the columns that are needed:\n",
    "tell_df = tell_df[['BA_Code', 'Time_UTC', 'Non-Transportation_Load_MWh']].copy()\n",
    "\n",
    "# Preview the TELL dataframe:\n",
    "tell_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d89c2d-a570-4a6e-a7f5-60bb4f50b839",
   "metadata": {},
   "source": [
    "## Merge the TELL and Transportation Dataframes Together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92f7ac-8475-4290-a94d-0460f0cc128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the trn_df and tell_df dataframes based on common \"BA_Code\" and \"Time_UTC\":\n",
    "merged_df = tell_df.merge(trn_df, on=['BA_Code', 'Time_UTC'])\n",
    "\n",
    "# Compute the sum of the transportation and non-transportation loads:\n",
    "merged_df['Total_Load_MWh'] = merged_df['Non-Transportation_Load_MWh'] + merged_df['Transportation_Load_MWh']\n",
    "\n",
    "# Round off the values to make the output file more readable:\n",
    "merged_df['Non-Transportation_Load_MWh'] = merged_df['Non-Transportation_Load_MWh'].round(2)\n",
    "merged_df['Transportation_Load_MWh'] = merged_df['Transportation_Load_MWh'].round(2)\n",
    "merged_df['Total_Load_MWh'] = merged_df['Total_Load_MWh'].round(2)\n",
    "\n",
    "# Fill in missing values with -9999:\n",
    "merged_df['Non-Transportation_Load_MWh'] = merged_df['Non-Transportation_Load_MWh'].fillna(-9999)\n",
    "merged_df['Transportation_Load_MWh'] = merged_df['Transportation_Load_MWh'].fillna(-9999)\n",
    "merged_df['Total_Load_MWh'] = merged_df['Total_Load_MWh'].fillna(-9999)\n",
    "\n",
    "# Rename the BA variable:\n",
    "merged_df.rename(columns={'BA_Code': 'BA'}, inplace=True) \n",
    "\n",
    "# Write out the dataframe to a .csv file:\n",
    "merged_df.to_csv((os.path.join(merged_data_output_dir, ('Total_Loads_' + gcam_scenario_to_process + '_' + year_to_process + '.csv'))), sep=',', index=False)\n",
    "\n",
    "# Preview the merged dataframe:\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a4e74d-c65d-4bc0-aae9-96c27da98018",
   "metadata": {},
   "source": [
    "## Format the Data for Ingest into GridView\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30565fc-1fc6-4f08-b7df-01ac04029019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gridview_data(merged_data_output_dir: str):\n",
    "    \n",
    "    # Read in the raw data .csv file:\n",
    "    gv_df = pd.read_csv((merged_data_output_dir + '2030_Load.csv'))\n",
    "    \n",
    "    # Subset to just the annual total demand by BA:\n",
    "    gv_df = gv_df[-3:-2]\n",
    "       \n",
    "    # Strip the unecessary bits from the column names:\n",
    "    gv_df.columns = gv_df.columns.str.replace(\"_CEC\", \"\", regex=True)\n",
    "    gv_df.columns = gv_df.columns.str.replace(\"_2030.dat\", \"\", regex=True)\n",
    "    gv_df.columns = gv_df.columns.str.replace(\"Load_\", \"\", regex=True)\n",
    "       \n",
    "    # Delete the index and last column:\n",
    "    del gv_df[\"Index\"], gv_df[\"Unnamed: 44\"]\n",
    "    \n",
    "    # Convert the values to floats:\n",
    "    gv_df = gv_df.astype('float64')\n",
    "    \n",
    "    # Compute the total loads for CISO, IPCO, NEVP, and PACE:\n",
    "    gv_df['CISO'] = (gv_df['CIPB'] + gv_df['CIPV'] + gv_df['CISC'] + gv_df['CISD'] + gv_df['VEA']).round(2)\n",
    "    gv_df['IPCO'] = (gv_df['IPFE'] + gv_df['IPMV'] + gv_df['IPTV']).round(2)\n",
    "    gv_df['PACE'] = (gv_df['PAID'] + gv_df['PAUT'] + gv_df['PAWY']).round(2)\n",
    "    gv_df['NEVP_Sum'] = (gv_df['NEVP'] + gv_df['SPPC']).round(2)\n",
    "           \n",
    "    # Rename a few columns for consistency:\n",
    "    gv_df.rename(columns={'CIPB': 'CISO_CIPB', 'CIPV': 'CISO_CIPV', 'CISC': 'CISO_CISC', 'CISD': 'CISO_CISD', 'VEA': 'CISO_VEA',\n",
    "                          'IPFE': 'IPCO_IPFE', 'IPMV': 'IPCO_IPMV', 'IPTV': 'IPCO_IPTV',\n",
    "                          'NEVP': 'NEVP_NEVP', 'SPPC': 'NEVP_SPPC',\n",
    "                          'PAID': 'PACE_PAID', 'PAUT': 'PACE_PAUT', 'PAWY': 'PACE_PAWY'}, inplace=True) \n",
    "    gv_df.rename(columns={'NEVP_Sum': 'NEVP'}, inplace=True) \n",
    "    \n",
    "    # Squeeze the dataframe:\n",
    "    gv_df = gv_df.squeeze().to_frame()\n",
    "        \n",
    "    # Rename the columns:\n",
    "    gv_df.reset_index(inplace=True)\n",
    "    gv_df = gv_df.rename(columns = {'index':'BA'})\n",
    "    gv_df.rename(columns={gv_df.columns[1]: \"Total_Load_MWh\" }, inplace = True)\n",
    "       \n",
    "    # Sort the dataframe alphabetically by BA name:\n",
    "    gv_df = gv_df.sort_values('BA')\n",
    "       \n",
    "    # Return the output dataframe:\n",
    "    return gv_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40122256-fdd3-4627-88d3-720591f158c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_loads_for_gridview(merged_data_output_dir: str):\n",
    "    \n",
    "    # Process the GridView file:\n",
    "    gv_df = process_gridview_data(merged_data_output_dir = merged_data_output_dir)\n",
    "    \n",
    "    # Compute the load fractions for the subregions:\n",
    "    CIPB_LF = (gv_df.loc[(gv_df['BA'] == 'CISO_CIPB')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'CISO')]['Total_Load_MWh'].values[0])\n",
    "    CIPV_LF = (gv_df.loc[(gv_df['BA'] == 'CISO_CIPV')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'CISO')]['Total_Load_MWh'].values[0])\n",
    "    CISC_LF = (gv_df.loc[(gv_df['BA'] == 'CISO_CISC')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'CISO')]['Total_Load_MWh'].values[0])\n",
    "    CISD_LF = (gv_df.loc[(gv_df['BA'] == 'CISO_CISD')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'CISO')]['Total_Load_MWh'].values[0])\n",
    "    VEA_LF  = (gv_df.loc[(gv_df['BA'] == 'CISO_VEA' )]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'CISO')]['Total_Load_MWh'].values[0])\n",
    "    IPFE_LF = (gv_df.loc[(gv_df['BA'] == 'IPCO_IPFE')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'IPCO')]['Total_Load_MWh'].values[0])\n",
    "    IPMV_LF = (gv_df.loc[(gv_df['BA'] == 'IPCO_IPMV')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'IPCO')]['Total_Load_MWh'].values[0])\n",
    "    IPTV_LF = (gv_df.loc[(gv_df['BA'] == 'IPCO_IPTV')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'IPCO')]['Total_Load_MWh'].values[0])\n",
    "    NEVP_LF = (gv_df.loc[(gv_df['BA'] == 'NEVP_NEVP')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'NEVP')]['Total_Load_MWh'].values[0])\n",
    "    SPPC_LF = (gv_df.loc[(gv_df['BA'] == 'NEVP_SPPC')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'NEVP')]['Total_Load_MWh'].values[0])\n",
    "    PAID_LF = (gv_df.loc[(gv_df['BA'] == 'PACE_PAID')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'PACE')]['Total_Load_MWh'].values[0])\n",
    "    PAUT_LF = (gv_df.loc[(gv_df['BA'] == 'PACE_PAUT')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'PACE')]['Total_Load_MWh'].values[0])\n",
    "    PAWY_LF = (gv_df.loc[(gv_df['BA'] == 'PACE_PAWY')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'PACE')]['Total_Load_MWh'].values[0])\n",
    "    \n",
    "    # Read in the merged total load file:\n",
    "    tell_df = pd.read_csv((os.path.join(merged_data_output_dir, ('Total_Loads_' + gcam_scenario_to_process + '_' + year_to_process + '.csv'))))\n",
    "     \n",
    "    # Compute the hours since the start of the year:\n",
    "    tell_df['Hour'] = ((pd.to_datetime(tell_df['Time_UTC']) - datetime.datetime(int(year_to_process), 1, 1, 0, 0, 0)) / np.timedelta64(1, 'h') + 1).astype(int)\n",
    "    \n",
    "    # Reshape the dataframe and reset the indexes:\n",
    "    load_df = tell_df.pivot(index = 'Hour', columns = 'BA', values = 'Total_Load_MWh')\n",
    "    load_df = load_df.reset_index(drop=False)\n",
    "    \n",
    "    # Add back in the text to the column headers:\n",
    "    load_df = load_df.add_suffix('_2030.dat')\n",
    "    load_df = load_df.add_prefix('Load_')\n",
    "    \n",
    "    # Rename the time variable:\n",
    "    load_df.rename(columns={'Load_Hour_2030.dat': 'Index'}, inplace=True)\n",
    "    \n",
    "    # Compute the loads for the subregions:\n",
    "    load_df['Load_CIPB_2030_CEC.dat'] = load_df['Load_CISO_2030.dat'] * CIPB_LF\n",
    "    load_df['Load_CIPV_2030_CEC.dat'] = load_df['Load_CISO_2030.dat'] * CIPV_LF\n",
    "    load_df['Load_CISC_2030_CEC.dat'] = load_df['Load_CISO_2030.dat'] * CISC_LF\n",
    "    load_df['Load_CISD_2030_CEC.dat'] = load_df['Load_CISO_2030.dat'] * CISD_LF\n",
    "    load_df['Load_VEA_2030.dat'] = load_df['Load_CISO_2030.dat'] * VEA_LF\n",
    "    load_df['Load_IPFE_2030.dat'] = load_df['Load_IPCO_2030.dat'] * IPFE_LF\n",
    "    load_df['Load_IPMV_2030.dat'] = load_df['Load_IPCO_2030.dat'] * IPMV_LF\n",
    "    load_df['Load_IPTV_2030.dat'] = load_df['Load_IPCO_2030.dat'] * IPTV_LF\n",
    "    load_df['Load_NEVP_Temp_2030.dat'] = load_df['Load_NEVP_2030.dat'] * NEVP_LF\n",
    "    load_df['Load_SPPC_2030.dat'] = load_df['Load_NEVP_2030.dat'] * SPPC_LF\n",
    "    load_df['Load_PAID_2030.dat'] = load_df['Load_PACE_2030.dat'] * PAID_LF\n",
    "    load_df['Load_PAUT_2030.dat'] = load_df['Load_PACE_2030.dat'] * PAUT_LF\n",
    "    load_df['Load_PAWY_2030.dat'] = load_df['Load_PACE_2030.dat'] * PAWY_LF\n",
    "    \n",
    "    # Drop the un-needed columns for BAs with subregions:\n",
    "    del load_df['Load_NEVP_2030.dat'], load_df['Load_CISO_2030.dat'], load_df['Load_IPCO_2030.dat'], load_df['Load_PACE_2030.dat']\n",
    "    \n",
    "    # Clean up the NEVP naming:\n",
    "    load_df.rename(columns={'Load_NEVP_Temp_2030.dat': 'Load_NEVP_2030.dat'}, inplace=True)\n",
    "    \n",
    "    # Read in the raw data GridView .csv file:\n",
    "    raw_gv_df = pd.read_csv((merged_data_output_dir + '2030_Load.csv'))\n",
    "    \n",
    "    # Delete the index column:\n",
    "    del raw_gv_df[\"Index\"] \n",
    "    \n",
    "    # Subset to just the rows we need:\n",
    "    raw_gv_df = raw_gv_df[1:8761]\n",
    "    \n",
    "    # Convert the values to floats:\n",
    "    raw_gv_df = raw_gv_df.astype('float64')\n",
    "    \n",
    "    # Shift the index by -1:\n",
    "    raw_gv_df = raw_gv_df.reset_index()\n",
    "    \n",
    "    # Merge in the GridView columns that aren't modeled by TELL:\n",
    "    load_df = pd.concat([load_df,raw_gv_df['Load_AESO_2030.dat']], axis=1)\n",
    "    load_df = pd.concat([load_df,raw_gv_df['Load_BCHA_2030.dat']], axis=1)\n",
    "    load_df = pd.concat([load_df,raw_gv_df['Load_CFE_2030.dat']], axis=1)\n",
    "    load_df = pd.concat([load_df,raw_gv_df['Load_TH_Malin_2030.dat']], axis=1)\n",
    "    load_df = pd.concat([load_df,raw_gv_df['Load_TH_Mead_2030.dat']], axis=1)\n",
    "    load_df = pd.concat([load_df,raw_gv_df['Load_TH_PV_2030.dat']], axis=1)\n",
    "    \n",
    "    # Compute the summary statistics:\n",
    "    stats_df = load_df.apply(['mean','sum','max','min'])\n",
    "    \n",
    "    # Fix the summary statistic labels:\n",
    "    stats_df.iloc[0, 0] = 'AVG'\n",
    "    stats_df.iloc[1, 0] = 'SUM'\n",
    "    stats_df.iloc[2, 0] = 'MAX'\n",
    "    stats_df.iloc[3, 0] = 'MIN'\n",
    "    \n",
    "    # Create a target dataframe with the spare hours:\n",
    "    target_df = pd.DataFrame({\"Index\": np.arange(1,8791,1)})\n",
    "    \n",
    "    # Merge load dataframe with the target dataframe:\n",
    "    merged_df = target_df.merge(load_df, on=['Index'], how='left')\n",
    "    \n",
    "    # Sort the data by column name and make the Index column appear first:\n",
    "    merged_df.rename(columns={'Index': 'AA'}, inplace=True)\n",
    "    merged_df = merged_df.sort_index(axis = 1)\n",
    "    merged_df.rename(columns={'AA': 'Index'}, inplace=True)\n",
    "    \n",
    "    # Add in a blank row and fill it with the year placeholder:\n",
    "    merged_df.loc[-0.5] = 0\n",
    "    merged_df = merged_df.sort_index().reset_index(drop=True)\n",
    "    merged_df.iloc[0, :] = '2030'\n",
    "    merged_df.at[0, 'Index'] = 'Year'\n",
    "    \n",
    "    # Merge the hourly load data and statistics dataframes together:\n",
    "    output_df = pd.concat([merged_df, stats_df], axis=0)\n",
    "    \n",
    "    # Replace NaNs with blank values:\n",
    "    output_df.replace(np.nan, \"\", regex=True)\n",
    "    \n",
    "    # Round all the numerical values to two decimals\n",
    "    output_df.round(2)\n",
    "    \n",
    "    # Write out the dataframe to a .csv file:\n",
    "    output_df.to_csv((os.path.join(merged_data_output_dir, ('GridView_Loads_' + gcam_scenario_to_process + '_' + year_to_process + '.csv'))), sep=',', index=False)\n",
    "    \n",
    "    # Return the output dataframe:\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71987691-72b0-4c47-b0fd-5d706d470533",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridview_df = format_loads_for_gridview(merged_data_output_dir = merged_data_output_dir)\n",
    "\n",
    "gridview_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9582c-6dfe-4650-a2dd-5c22c7050246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9.15_std",
   "language": "python",
   "name": "py3.9.15_std"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
